{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# import re\n",
    "import google.generativeai as genai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web = webdriver.Chrome()\n",
    "web.get('https://forms.gle/nm5JjrN6BHiiKFjVA')\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"MY_API_KEY\")\n",
    "generation_config = {\"temperature\": 0.9, \"top_p\": 1, \"top_k\": 1, \"max_output_tokens\": 2048}\n",
    "model = genai.GenerativeModel(\"gemini-pro\", generation_config = generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Auto Google form filler\n",
    "\n",
    "def fill_form(sname,company,date,head,con,sum,radio):\n",
    "    \n",
    "    global names\n",
    "    global namec\n",
    "    global named\n",
    "    global nameh\n",
    "    global cname\n",
    "    global namesum\n",
    "    \n",
    "\n",
    "    names = web.find_element('xpath', '//*[@id=\"mG61Hd\"]/div[2]/div/div[2]/div[1]/div/div/div[2]/div/div[1]/div/div[1]/input')\n",
    "    names.send_keys(sname)\n",
    "\n",
    "    namec = web.find_element('xpath', '//*[@id=\"mG61Hd\"]/div[2]/div/div[2]/div[2]/div/div/div[2]/div/div[1]/div/div[1]/input')\n",
    "    namec.send_keys(company)\n",
    "\n",
    "    named = web.find_element('xpath', '//*[@id=\"mG61Hd\"]/div[2]/div/div[2]/div[3]/div/div/div[2]/div/div/div[2]/div[1]/div/div[1]/input')\n",
    "    named.send_keys(date)\n",
    "\n",
    "    nameh = web.find_element('xpath', '//*[@id=\"mG61Hd\"]/div[2]/div/div[2]/div[4]/div/div/div[2]/div/div[1]/div/div[1]/input')\n",
    "    nameh.send_keys(head)\n",
    "\n",
    "    cname = web.find_element('xpath', '//*[@id=\"mG61Hd\"]/div[2]/div/div[2]/div[5]/div/div/div[2]/div/div[1]/div[2]/textarea')\n",
    "    cname.send_keys(con)\n",
    "\n",
    "    namesum = web.find_element('xpath', '//*[@id=\"mG61Hd\"]/div[2]/div/div[2]/div[6]/div/div/div[2]/div/div[1]/div[2]/textarea')\n",
    "    namesum.send_keys(sum)\n",
    "\n",
    "    # time.sleep(2)\n",
    "    button = web.find_elements(By.CLASS_NAME, \"Od2TWd\")\n",
    "    for rad in button:\n",
    "        # if rad.get_attribute('data-value') == \"Politics\":\n",
    "        #     rad.click()\n",
    "        if rad.get_attribute('data-value') == radio:\n",
    "            rad.click()\n",
    "    # time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        namesubmit = web.find_element('xpath','//*[@id=\"mG61Hd\"]/div[2]/div/div[3]/div[1]/div[1]/div/span/span')\n",
    "        namesubmit.click()\n",
    "\n",
    "        time.sleep(1)\n",
    "        another_one = web.find_element('xpath','/html/body/div[1]/div[2]/div[1]/div/div[4]/a')\n",
    "        another_one.click()\n",
    "        time.sleep(1)\n",
    "        # names.clear()\n",
    "        # namec.clear()\n",
    "        # named.clear()\n",
    "        # nameh.clear()\n",
    "        # cname.clear()\n",
    "        # namesum.clear()\n",
    "\n",
    "    except:\n",
    "        print(head)\n",
    "        names.clear()\n",
    "        namec.clear()\n",
    "        named.clear()\n",
    "        nameh.clear()\n",
    "        cname.clear()\n",
    "        namesum.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://timesofindia.indiatimes.com/2013/5/24/archivelist/year-2013,month-5,starttime-41418.cms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)\n",
    "html_content = r.text\n",
    "soup = BeautifulSoup(html_content, 'lxml')  \n",
    "links = [a.get('href') for a in soup.find_all('a', href=True)]\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [k for k in links if 'http://timesofindia.indiatimes.com' in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = [x for x in urls if x not in done]\n",
    "len(new)\n",
    "len(done)\n",
    "## News Extraction\n",
    "\n",
    "i = 0\n",
    "for url in new:\n",
    "    # i+=1\n",
    "    done.append(url)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find('h1').text\n",
    "    # try:\n",
    "    #     title = soup.find('h1', class_='HNMDR').text\n",
    "    # except:\n",
    "    #     continue\n",
    "    # date = soup.find('div', class_='xf8Pm byline').text\n",
    "    # date = re.search(r'\\w+ \\d{1,2}, \\d{4}', date).group(0) \n",
    "    text = soup.find('div',class_='_s30J clearfix').text\n",
    "    # text = ''\n",
    "    # for para in soup.find_all('p'):\n",
    "    #     text += para.text + '\\n\\n'\n",
    "\n",
    "    ## Summarization using Gemini (Bard)\n",
    "\n",
    "    time.sleep\n",
    "    try:\n",
    "        response = model.generate_content(\"Summarize the following text in 100 words: \"+ text)\n",
    "        context = response.text\n",
    "        # print('resdone')\n",
    "    except:\n",
    "        context = \"\"\n",
    "        print(title)\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        newscat = model.generate_content(\"Categorize this news into just one (only one) of the most appropriate field representing the news ['Politics','International News','National News','Local News','Politics','Business and Finance','Science and Technology','Health and Wellness','Entertainment','Sports','Lifestyle and Features','Opinion and Editorial','Environment','Education','Crime and Justice','Human Interest', 'Obituaries','Weather','Religion and Spirituality','Technology and Gadgets','Automotive']: \"+ text)\n",
    "        newscat= newscat.text\n",
    "        # print('newdone')\n",
    "\n",
    "    except:\n",
    "        newscat = \"\" \n",
    "        print(title)\n",
    "        continue\n",
    "\n",
    "    sname = 'Mann Kurani'\n",
    "    company = 'The Times of India'\n",
    "    date = '20/07/2022'\n",
    "    head= title\n",
    "    con= text\n",
    "    sum= context\n",
    "    radio = newscat\n",
    "    # print(radio)\n",
    "    fill_form(sname,company,date,head,con,sum,radio)\n",
    "    \n",
    "    i+=1\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
